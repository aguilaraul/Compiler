# Compiler
 
##### Background
A typical compiler consists of two main modules: _syntax analysis_ and _code generation_.
The syntax analysis task is usually divided further into two modules: _tokenizing_, or
grouping, of input characters into language atoms, and _parsing_, or attempting to match the
resulting atoms stream to the syntax rules of the underlying language. We focus on the
_syntax analyzer_ module of the compiler, whose job is "understanding the structure of a
program." To understand -- _parse_ -- a given program means to determine the exact
correspondence between the program's text and the grammar's rules. In order to do so, we
first have to transform the program's text into a list of _tokens_.

##### Lexical Analysis
In it's plainest syntactic form, a program is simply a sequence of characters, stored in a
text file. The first step in the syntax analysis of a program is to group the characters
into _tokens_ (as defined by the language syntax), while ignoring white space and comments.
This step is usually called _lexical analysis_, _scanning_, or _tokenizing_. Once a program
has been tokenized, the tokens (rather than the characters) are viewed as its basic atoms,
and the tokens stream becomes the main input of the compiler.

##### Grammars
Once we have lexically analyzed a program into a stream of tokens, we now face the more
challenging task of parsing the tokens stream into a formal structure. These groupings and
classification tasks can be done by attempting to match the tokens stream on some predefined
set of rules known as a grammar.

Almost all programming languages can be specified using formalisms known as _context-free
grammars_. A context-free grammar is a set of rules specifying how syntactic elements in
some language can be formed from simpler ones. Indeed, each grammar has a dual perspective.
From a declarative standpoint, the grammar specifies allowable ways to combine tokens, also
called _terminals_, into higher-level syntactic elements, also called _non-terminals_. From
an analytical standpoint, the grammar is a prescription for doing the reverse: parsing a
given input (set of tokens resulting from the tokenizing phase) into non-terminals,
lower-level non-terminals, and eventually terminals that cannot be decomposed any further.

##### Parsing
The act of checking whether a grammar "accepts" an input text as valid is called _parsing_.
Parsing a given text means determining the exact correspondence between the text and the
rules of a given grammar. Since grammar rules are hierarchical, the output generated by the
parser can be described in a tree-oriented data structure called a _parse tree_ or a
_derivation tree_.

As a side effect of the parsing process, the entire syntactic structure of the input text is
uncovered. Some compilers represent this tree by an explicit data structure that is further
used for code generation and error reporting. Other compilers represent the programs's
structure implicitly, generating code and reporting errors on the fly. Such compilers don't
have to hold the entire program in memory, but only the subtree associated with the
presently parsed element.